{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPrlS4SDN4HQEdXQ2ho/aJX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Kendi train_test_split fonksiyonumu oluşturma sebebim, verinin zaman serisinden oluşması. Eğer train_test_split'i kullansaydım rastgele bir şekilde veriyi dağıtıp, zaman serisini ve ardışıklığı bozacaktı."],"metadata":{"id":"lqC0WHTkCtaw"}},{"cell_type":"code","source":["def train_test_split(train,percent = 0.8):\n","  n_rows = train.shape[0]\n","  df_train = train.iloc[:int(percent*n_rows), :]\n","  df_test = train.iloc[int(percent*n_rows):, :]\n","  X_train = df_train.drop(\"d_enerji\",axis = 1)\n","  y_train = df_train[\"d_enerji\"]\n","  X_test = df_test.drop(\"d_enerji\",axis = 1)\n","  y_test = df_test[\"d_enerji\"] #y_true\n","  return X_train, y_train, X_test, y_test"],"metadata":{"id":"90U7u-HZ-4Xs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Polynomial Regression"],"metadata":{"id":"F-x5MtYml5SL"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"OF8jiWDvly9O"},"outputs":[],"source":["def polynomialRegression(train,sub_df):\n","  test_dict = hyperParameterOptimizationForPolynomialRegression(train)\n","  min_key = min(test_dict.keys())\n","  print(\"MAPE (Mean Absolute Percentage Error)= \", min_key)\n","  best_parameters = test_dict.get(min_key)\n","  degree = best_parameters[\"degree\"]\n","  print(degree)\n","  import matplotlib.pyplot as plt\n","  from sklearn.linear_model import LinearRegression\n","  from sklearn.preprocessing import PolynomialFeatures\n","  X = train.drop(\"d_enerji\",axis = 1)\n","  y = train[\"d_enerji\"]\n","\n","  poly = PolynomialFeatures(degree = degree)\n","  x_poly = poly.fit_transform(X)\n","  lin_reg = LinearRegression()\n","  lin_reg.fit(x_poly,y)\n","\n","  x_sub = sub_df.drop(\"d_enerji\", axis=1)\n","  x_sub_poly = poly.transform(x_sub)\n","  y_pred = lin_reg.predict(x_sub_poly)\n","  return y_pred\n","\n","def hyperParameterOptimizationForPolynomialRegression(train):\n","  X_train, y_train, X_test, y_test = train_test_split(train)\n","  from sklearn.linear_model import LinearRegression\n","  from sklearn.preprocessing import PolynomialFeatures\n","  degree_list = [2,3]\n","  results_dict = {}\n","  for degree in degree_list:\n","    poly = PolynomialFeatures(degree = degree)\n","    x_poly = poly.fit_transform(X_train)\n","    lin_reg = LinearRegression()\n","    lin_reg.fit(x_poly,y_train)\n","    x_test_poly = poly.transform(X_test)\n","    y_pred = lin_reg.predict(x_test_poly)\n","\n","    mape_result = mape(y_test,y_pred)\n","    temp_dict = { mape_result: {\"degree\" : degree} }\n","    results_dict.update(temp_dict)\n","\n","  return results_dict"]},{"cell_type":"markdown","source":["RandomForestRegressor"],"metadata":{"id":"V0DoDiGyl9aC"}},{"cell_type":"code","source":["def hyperParameterOptimizationForRandomForestRegressor(train):\n","  from sklearn.ensemble import RandomForestRegressor\n","  from sklearn.metrics import mean_squared_error\n","  X_train, y_train, X_test, y_test = train_test_split(train)\n","\n","  n_estimators_list = [100,250,500] # Karar ağacı sayısı. Genellikle 100-500 arasında bir değer seçilir.\n","  max_features_list = [] # max_features: Ağacın dallanması için göz önünde bulundurulacak özellik sayısı. Bu parametre için genellikle sqrt (karekök), log2 (logaritma tabanı 2) veya bir sabit değer kullanılır.\n","  # max_depth_list = [] # Karar ağacının maksimum derinliği. Bu parametre için genellikle None (sınırsız derinlik), 10 veya 20 gibi bir değer kullanılır.\n","  min_samples_split_list = [2,3,5] # min_samples_split: Dallanmanın gerçekleşmesi için minimum örnek sayısı. Bu parametre için genellikle 2-5 arasında bir değer kullanılır.\n","  min_samples_leaf_list = [2,3,4] # min_samples_leaf: Bir yaprağın oluşması için minimum örnek sayısı. Bu parametre için genellikle 1-5 arasında bir değer kullanılır.\n","  results_dict = {}\n","  for n_estimators in n_estimators_list:\n","    for min_samples_split in min_samples_split_list:\n","      for min_samples_leaf in min_samples_leaf_list:\n","        rf_reg = RandomForestRegressor(n_estimators=n_estimators, min_samples_split = min_samples_split, min_samples_leaf = min_samples_leaf,random_state=42)\n","        rf_reg.fit(X_train, y_train)\n","        y_pred = rf_reg.predict(X_test)\n","        mape_result = mape(y_test,y_pred)\n","        temp_dict = { mape_result: {\"n_estimators\" : n_estimators,\"min_samples_split\" : min_samples_split,\"min_samples_leaf\" : min_samples_leaf} }\n","        results_dict.update(temp_dict)\n","  return results_dict\n","\n","def RandomForestRegressor(train,sub_df):\n","  from sklearn.ensemble import RandomForestRegressor\n","  X = train.drop(\"d_enerji\",axis = 1)\n","  y = train[\"d_enerji\"]\n","  test_dict = hyperParameterOptimizationForRandomForestRegressor(train)\n","  min_key = min(test_dict.keys())\n","  print(\"MAPE (Mean Absolute Percentage Error)= \", min_key)\n","  best_parameters = test_dict.get(min_key)\n","  n_estimators = best_parameters[\"n_estimators\"]\n","  min_samples_split = best_parameters[\"min_samples_split\"]\n","  min_samples_leaf = best_parameters[\"min_samples_leaf\"]\n","  print(n_estimators,min_samples_split,min_samples_leaf)\n","  rf_reg = RandomForestRegressor(n_estimators= n_estimators, min_samples_split = min_samples_split, min_samples_leaf = min_samples_leaf,random_state=42)\n","  rf_reg.fit(X, y)\n","  y_pred = rf_reg.predict(sub_df.drop(\"d_enerji\",axis=1))\n","  return y_pred"],"metadata":{"id":"dmarKgUjmAAA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["GradientBoostRegressor"],"metadata":{"id":"t-Z4YMlpmBbI"}},{"cell_type":"code","source":["def hyperParameterOptimizationForGradientBoostingRegressor(train):\n","  from sklearn.ensemble import GradientBoostingRegressor\n","  from sklearn.metrics import mean_squared_error\n","  X_train, y_train, X_test, y_test = train_test_split(train)\n","  n_estimators_list = [250,500] # Karar ağacı sayısı. Genellikle 100-500 arasında bir değer seçilir.\n","  learning_rate_list = [0.1, 0.01, 0.05]\n","  max_depth_list = [3,4,5] #  Her ağacın maksimum derinliğini belirler. Daha derin ağaçlar, daha karmaşık modeller oluşturur ancak aynı zamanda aşırı uyuma neden olabilir. Genellikle, 3-5 arası değerler kullanılır.\n","  min_samples_split_list = [5,7] # Her iç düğümün ayrılması için minimum örnek sayısını belirler. Daha yüksek bir değer, daha basit modeller oluşturur ancak aynı zamanda daha az esneklik sağlar. Genellikle, 2-10 arası değerler kullanılır.\n","  # subsample_list = [0.5,0.75] # subsample: Eğitim örneklerinin alt kümesini seçmenizi sağlar. Bu, ağaçlar arasındaki çeşitliliği artırır ve aşırı uyum riskini azaltabilir. Genellikle, 0.5-1 arası değerler kullanılır.\n","  results_dict = {}\n","  for n_estimators in n_estimators_list:\n","    for learning_rate in learning_rate_list:\n","      for max_depth in max_depth_list:\n","        for min_samples_split in min_samples_split_list:\n","          rf_reg = GradientBoostingRegressor(n_estimators=n_estimators, min_samples_split = min_samples_split, learning_rate = learning_rate, max_depth = max_depth)\n","          rf_reg.fit(X_train, y_train)\n","          y_pred = rf_reg.predict(X_test)\n","          mape_result = mape(y_test,y_pred)\n","          temp_dict = { mape_result: {\"n_estimators\": n_estimators, \"min_samples_split\" : min_samples_split, \"learning_rate\" : learning_rate, \"max_depth\" : max_depth} }\n","          results_dict.update(temp_dict)\n","  return results_dict\n","\n","def GradientBoostingRegressor(train,sub_df):\n","  from sklearn.ensemble import GradientBoostingRegressor\n","  X = train.drop(\"d_enerji\",axis = 1)\n","  y = train[\"d_enerji\"]\n","  test_dict = hyperParameterOptimizationForGradientBoostingRegressor(train)\n","  min_key = min(test_dict.keys())\n","  print(\"MAPE (Mean Absolute Percentage Error)= \", min_key)\n","  best_parameters = test_dict.get(min_key)\n","  n_estimators = best_parameters[\"n_estimators\"]\n","  min_samples_split = best_parameters[\"min_samples_split\"]\n","  learning_rate = best_parameters[\"learning_rate\"]\n","  max_depth = best_parameters[\"max_depth\"]\n","  print(n_estimators,min_samples_split,learning_rate,max_depth)\n","  gbr = GradientBoostingRegressor(n_estimators=n_estimators,min_samples_split=min_samples_split, learning_rate=learning_rate, max_depth=max_depth)\n","  gbr.fit(X, y)\n","  y_pred = gbr.predict(sub_df.drop(\"d_enerji\",axis=1))\n","  return y_pred"],"metadata":{"id":"CJiWVWVbmEgu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["XGBoost"],"metadata":{"id":"L_ANyJOlmFi-"}},{"cell_type":"code","source":["def mape(y_true, y_pred): # MAPE (Mean Absolute Percentage Error)\n","    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n","\n","def hyperParameterOptimizationForXGBoost(train):\n","  X_train, y_train, X_test, y_test = train_test_split(train)\n","  from xgboost import XGBRegressor\n","  max_depth_list = [3,5,7] # Karar ağacıların maksimum derinliği. 3-10 arası bir değer genellikle önerilir.\n","  learning_rate_list = [0.1, 0.01, 0.05] # Her ağaç oluşturma işleminde kullanılan öğrenme oranı. 0.01-0.1 arası bir değer genellikle önerilir.\n","  n_estimators_list = [100,250,500,1000] # Oluşturulacak ağaç sayısı. 50-500 arası bir değer genellikle önerilir. 50,100,250,500,1000\n","  results_dict = {}\n","  for max_depth in max_depth_list:\n","    for learning_rate in learning_rate_list:\n","      for n_estimator in n_estimators_list:\n","        xgb_model = XGBRegressor(max_depth = max_depth, n_estimators = n_estimator, learning_rate= learning_rate)\n","        xgb_model.fit(X_train,y_train)\n","        y_pred = xgb_model.predict(X_test)\n","        mape_result = mape(y_test,y_pred)\n","        temp_dict = { mape_result: {\"max_depth\" : max_depth,\"learning_rate\" : learning_rate,\"n_estimators\" : n_estimator}}\n","        results_dict.update(temp_dict)\n","  return results_dict\n","\n","def bestParamsForXGBoost(train):\n","  test_dict = hyperParameterOptimizationForXGBoost(train)\n","  min_key = min(test_dict.keys())\n","  print(\"MAPE (Mean Absolute Percentage Error)= \", min_key)\n","  best_parameters = test_dict.get(min_key)\n","  max_depth = best_parameters[\"max_depth\"]\n","  learning_rate = best_parameters[\"learning_rate\"]\n","  n_estimators = best_parameters[\"n_estimators\"]\n","  print(f\"max_depth ={max_depth}, learning_rate = {learning_rate}, n_estimators = {n_estimators}\")\n","  return max_depth,learning_rate,n_estimators\n","\n","def gridSearchCVForXGBoost(train):\n","  from sklearn.model_selection import GridSearchCV\n","  from xgboost import XGBRegressor\n","  from sklearn.metrics import make_scorer\n","  from sklearn.metrics import mean_absolute_percentage_error\n","  X_train, y_train, X_test, y_test = train_test_split(train)\n","  scorer = make_scorer(mean_absolute_percentage_error, greater_is_better=False)\n","  model = XGBRegressor()\n","  params = {\n","    'n_estimators': [100,250,500,1000],\n","    'max_depth': [3, 5, 7],\n","    'learning_rate': [0.1, 0.01, 0.05]\n","  }\n","  grid = GridSearchCV(\n","      estimator = model, \n","      param_grid = params, \n","      scoring=scorer,\n","      n_jobs=-1,\n","      verbose=3,\n","      cv=5)\n","  grid.fit(X_train, y_train)\n","  thebestresult = grid.best_score_\n","  print(thebestresult)\n","  print(\"En iyi parametreler:\", grid.best_params_)\n","  # best_model = grid.best_estimator_\n","  # best_model.fit(X_train, y_train)\n","\n","def XGBoost(train_data,sub_df):\n","  from xgboost import XGBRegressor\n","  max_depth, learning_rate, n_estimators = bestParamsForXGBoost(train)\n","  xgb_model = XGBRegressor(max_depth = max_depth, n_estimators = n_estimators, learning_rate= learning_rate)\n","  xgb_model.fit(train_data.drop(\"d_enerji\",axis = 1),train_data[\"d_enerji\"])\n","  y_pred = xgb_model.predict(sub_df.drop(\"d_enerji\",axis = 1))\n","  return y_pred"],"metadata":{"id":"g1D2mmmYmJok"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["sarimax"],"metadata":{"id":"J6wM3jJfmS4u"}},{"cell_type":"markdown","source":["p = 1-3, d = 0-2, q = 1-3: Bu, otoregresif (AR), fark (I) ve hareketli ortalama (MA) terimlerinin sayısı için tipik bir aralıktır.\n","\n","P = 0-3, D = 0-1, Q = 0-3, s = 12: Bu, sezonluk AR, fark ve hareketli ortalama terimleri için tipik bir aralıktır. s, verilerin sezonluk bir desen içerip içermediğine bağlı olarak değişebilir.\n","\n","exog: Eğer SARIMAX modelinde eksojen değişkenler varsa, bunların sayısı ve türü de modelin amacına bağlı olarak değişebilir."],"metadata":{"id":"n9_3v21Tm8LT"}},{"cell_type":"code","source":["def sarimax():\n","  from statsmodels.tsa.statespace.sarimax import SARIMAX\n","  # model = SARIMAX( endog = train[\"d_enerji\"] , order=(p, d, q), seasonal_order=(P, D, Q, S))\n","  model = SARIMAX( endog = train[\"d_enerji\"] , order=(1, 1, 1), seasonal_order=(1, 1, 1, 12)) # P = \n","  result = model.fit()\n","  predictions = result.predict(start = sub.index[0], end = sub.index[-1])\n","  return predictions"],"metadata":{"id":"H6ulaM3WmTma"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def prophet(train,sub_df,holidays):\n","  train_for_model = train.copy()\n","  train_for_model = train_for_model.rename(columns = {\n","      \"tarihwithhours\":\"ds\",\n","      \"d_enerji\":\"y\"\n","  })\n","  sub_for_model = sub_df.copy()\n","  sub_for_model = sub_for_model.rename(columns = {\"tarihwithhours\":\"ds\"})\n","\n","  from prophet import Prophet # 5.54701, hiçbir seasonality eklemeden.\n","  model = Prophet(holidays = holidays)\n","  model.add_seasonality(name='weekly', period=7, fourier_order=5, condition_name='is_weekend')\n","  model.add_seasonality(name='yearly', period=365.25/4, fourier_order=10, condition_name='is_spring')\n","  model.add_seasonality(name='yearly', period=365.25/4, fourier_order=10, condition_name='is_summer')\n","  model.add_seasonality(name='yearly', period=365.25/4, fourier_order=10, condition_name='is_fall')\n","  model.add_seasonality(name='yearly', period=365.25/4, fourier_order=10, condition_name='is_winter')\n","  model.add_seasonality(name='monthly', period=30.5, fourier_order=5, mode='additive') #\n","  model.add_seasonality(name='yearly', period=365.25, fourier_order=10, mode='additive')\n","  model.add_seasonality(name='hourly', period=24, fourier_order=5)\n","\n","  model.fit(train_for_model)\n","\n","  forecast = model.predict(sub_for_model)\n","  y_pred = forecast[\"yhat\"]\n","  return y_pred"],"metadata":{"id":"d5U62VwI-9av"},"execution_count":null,"outputs":[]}]}